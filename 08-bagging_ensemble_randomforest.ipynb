{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "dataset = load_breast_cancer()\n",
    "X = dataset.data\n",
    "y = dataset.target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dataset.data, columns=dataset.feature_names)\n",
    "\n",
    "# Add the target variable to the dataframe\n",
    "df['target'] = dataset.target\n",
    "\n",
    "# Print the first 5 rows of the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.pop(\"target\")\n",
    "X = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "knn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# TODO: Check sklearn documentation on what is a OOB score and what is the reason for using it?\n",
    "\n",
    "bag_knn = BaggingClassifier(KNeighborsClassifier(n_neighbors=5),\n",
    "                            n_estimators=10, max_samples=0.5,\n",
    "                            bootstrap=True, random_state=3,oob_score=True) \n",
    "\n",
    "\n",
    "bag_knn.fit(X_train, y_train)\n",
    "bag_knn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model_dt = DecisionTreeClassifier(min_samples_split=2)\n",
    "model_dt.fit(X_train,y_train)\n",
    "model_dt.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model_dt = DecisionTreeClassifier(criterion='entropy', max_depth=24, min_samples_leaf=1)\n",
    "model_dt.fit(X_train,y_train)\n",
    "model_dt.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model_rf = RandomForestClassifier(random_state=6)\n",
    "model_rf.fit(X_train,y_train)\n",
    "model_rf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_param = {\n",
    "    \"n_estimators\" : [90,100],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth' : range(5,10,1),\n",
    "    'min_samples_leaf' : range(1,4,1),\n",
    "    'min_samples_split': range(2,5,1),\n",
    "    'max_features' : ['auto','log2']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid_search = GridSearchCV(estimator=model_rf, param_grid=grid_param, cv=3, n_jobs=-1, verbose=3)\n",
    "grid_search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_best_rf = RandomForestClassifier(random_state=6, **grid_search.best_params_)\n",
    "model_best_rf.fit(X_train,y_train)\n",
    "\n",
    "model_best_rf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_best_rf.n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model_best_rf.estimators_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_best_rf.estimators_[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "plt.figure(figsize=(80,40))\n",
    "plot_tree(model_best_rf.estimators_[2], feature_names = dataset.feature_names,class_names=['Cancer', \"No Cancer\"],filled=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Importance\n",
    "\n",
    "The Random Forest algorithm has built-in feature importance which can be computed in two ways:\n",
    "1. Gini importance (or mean decrease impurity)\n",
    "2. Mean Decrease Accuracy - is a method of computing the feature importance on permuted out-of-bag (OOB) samples based on mean decrease in the accuracy. This method is not implemented in the scikit-learn package\n",
    "\n",
    "Gini importance is computed from Random Forest structure. In the Rf construction, we can measure how each feature decreases the impurity of the split. For each feature we can collect how on average it decreases the impurity. The average over all trees in the forest is the measure of the feature importance. The drawbacks of the method is to tendency to prefer numerical features and categorical features with high cardinality. For correlated features, it may even select one of the feature and neglect the importance of the second one leading to wrong conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.feature_names)\n",
    "print(model_best_rf.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.barh(dataset.feature_names, model_best_rf.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_idx = model_best_rf.feature_importances_.argsort()\n",
    "plt.barh(dataset.feature_names[sorted_idx], model_best_rf.feature_importances_[sorted_idx])\n",
    "plt.xlabel(\"Random Forest Feature Importance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Permutation based feature importance\n",
    "\n",
    "Permutation based importance can be used to overcome drawbacks of default feature importance computed with mean impurity decrease. This method randomly shuffles each feature and compute the change in the modelâ€™s performance. The features impacting performance the most are the most important ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "perm_importance = permutation_importance(model_best_rf, X_test, y_test)\n",
    "sorted_idx = perm_importance.importances_mean.argsort()\n",
    "plt.barh(dataset.feature_names[sorted_idx], perm_importance.importances_mean[sorted_idx])\n",
    "plt.xlabel(\"Permutation Importance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SHAP based feature importance\n",
    "\n",
    "Shapley values are calculated using Cooperative Game Theory. Gives feature importance breakdown per class (label) also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap # Requires SHAP install in your environment\n",
    "\n",
    "explainer = shap.TreeExplainer(model_best_rf)\n",
    "shap_values = explainer.shap_values(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_test, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.bar(shap_values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choosen_instance = X_test.iloc[[0]]\n",
    "shap_values_inst = explainer.shap_values(choosen_instance)\n",
    "shap.initjs()\n",
    "shap.force_plot(explainer.expected_value[1], shap_values_inst[1], choosen_instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is a force plot for feature shapley values. Feature values in pink cause to increase the prediction. Size of the bar shows the magnitude of the feature's effect. Feature values in blue cause to decrease the prediction. Sum of all feature SHAP values explain why model prediction was different from the baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = model_best_rf.predict(X_test)\n",
    "accuracy_score(y_pred=y_pred, y_true=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, fbeta_score\n",
    "\n",
    "print(f\"Precision = {precision_score(y_test, y_pred)}\")\n",
    "print(f\"Recall = {recall_score(y_test, y_pred)}\")\n",
    "print(f\"F-1 score = {f1_score(y_test, y_pred)}\")\n",
    "print(f\"F-2 score = {fbeta_score(y_test, y_pred, beta=2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision_recall_fscore_support(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# macro average is arithmetic mean of per class scores\n",
    "# Read this: https://towardsdatascience.com/micro-macro-weighted-averages-of-f1-score-clearly-explained-b603420b292f \n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluating Random Forest model performance and robustness with ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict probabilities instead of 1/0 label\n",
    "y_pred_prob = model_best_rf.predict_proba(X_test)\n",
    "y_pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob[:,1] > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y_pred_prob[:,1] > 0.7).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, (y_pred_prob[:,1] > 0.5).astype(int))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpretation of this result after increasing the threshold \n",
    "# Increasing threshold decreases FP, but increases FN such that FP+FN remains constant for a given model+hyperparams\n",
    "threshold = 0.7\n",
    "cm = confusion_matrix(y_test, (y_pred_prob[:,1] > threshold).astype(int))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![TPFP](https://onedrive.live.com/embed?resid=A5A4158EF1352FCB%211958&authkey=%21AAtvfe4lv5jfOGg&width=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc, roc_curve\n",
    "fpr, tpr, threshold = roc_curve(y_test, model_best_rf.predict_proba(X_test)[:, 1])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import RocCurveDisplay\n",
    "svc_disp = RocCurveDisplay.from_estimator(model_best_rf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise: Draw a PR curve and understand it\n",
    "\n",
    "Dont blindly copy code from ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imbalanced Dataset classification\n",
    "\n",
    "This is not part of syllabus and is a optional exercise. Refer to this kaggle credit card fraud dataset and look at the provided notebooks\n",
    "1. https://www.kaggle.com/code/janiobachmann/credit-fraud-dealing-with-imbalanced-datasets\n",
    "2. https://www.kaggle.com/code/marcinrutecki/best-techniques-and-metrics-for-imbalanced-dataset\n",
    "\n",
    "pip install imblearn for SMOTE implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi class classification\n",
    "\n",
    "1. Read this article to understand the classification report especially in the case of multi class classification: https://towardsdatascience.com/micro-macro-weighted-averages-of-f1-score-clearly-explained-b603420b292f\n",
    "2. Download a kaggle dataset for multi class classification. \n",
    "3. Your goal is to run all the evaluation metrics, ROC AUC curves and classification reports and see the impact of changing threshold on False Positive and False negatives\n",
    "4. Dont use iris dataset because the distributions are very cleanly separated and there wont be any real False positive and False negatives\n",
    "\n",
    "References\n",
    "1. https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py\n",
    "2. Multiclass ROC plots https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html\n",
    "\n",
    "Read about One Versus One (OvO) and One versus Rest (OvR)\n",
    "https://towardsdatascience.com/comprehensive-guide-to-multiclass-classification-with-sklearn-127cc500f362"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking (aka Stacked Generalization)\n",
    "\n",
    "Read up on Stacking Generalization and meta models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "\n",
    "dataset = load_wine()\n",
    "X = dataset.data\n",
    "y = dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dataset.data, columns=dataset.feature_names)\n",
    "\n",
    "# Add the target variable to the dataframe\n",
    "df['target'] = dataset.target\n",
    "\n",
    "# Print the first 5 rows of the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.pop(\"target\")\n",
    "X = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=7)\n",
    "\n",
    "knn.fit(X_train,y_train)\n",
    "knn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC()\n",
    "svm.fit(X_train,y_train)\n",
    "svm.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=7)\n",
    "svm = SVC()\n",
    "lr = LogisticRegression()\n",
    "stacking = StackingClassifier(estimators=[('knn', knn), (\"svc\", svm)], final_estimator=lr)\n",
    "stacking.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = stacking.predict(X_test)\n",
    "accuracy = stacking.score(X_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quickstart",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
