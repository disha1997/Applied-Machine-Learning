{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "**Prep**\n",
    "1. Load dataset\n",
    "2. Simulate iris dataset with Nan\n",
    "\n",
    "**Simple Imputations on Iris dataset** \n",
    "1. Nan check in dataframe\n",
    "2. Duplicate check & immediate deletion\n",
    "3. Delete features with zero or low variance\n",
    "4. Option 1 - Null value deletion\n",
    "5. Option 2 - Mean/Median/Mode imputation\n",
    "6. Option 3 - Grouped Mean imputation\n",
    "7. kNN imputation\n",
    "8. Multivariate imputations are better - seeing in action\n",
    "9. Multiple rounds of imputation\n",
    "\n",
    "\n",
    "**Analyzing income dataset**\n",
    "1. Deduplication & Nan Imputation\n",
    "2. Dealing with numerical columns - Imputation & Scaling\n",
    "3. Dealing with categorical variables\n",
    "4. Combine all features back together - Categorical and numerical\n",
    "5. Apply KNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"numpy version = {np.__version__}\")\n",
    "print(f\"pandas version = {pd.__version__}\") \n",
    "print(f\"sklearn version = {sk.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import load_iris function from datasets module\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "def get_iris_df(concatenate_Xy=False):\n",
    "    iris = load_iris()\n",
    "\n",
    "    df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "    y = iris.target\n",
    "\n",
    "    print(f\"Creating Iris dataframe with {df.shape} records and {y.shape} target records\")\n",
    "\n",
    "    if concatenate_Xy is True:\n",
    "        df[\"target\"] = y\n",
    "        return df\n",
    "\n",
    "    return df, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, y = get_iris_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate a dataset with Nan\n",
    "\n",
    "Setting random sepal length & width to be Nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some random index. We will set sepal length in those rows to be Nan\n",
    "sepal_len_nan_row_idx = np.random.randint(low=0, high=df.shape[0], size=10)\n",
    "sepal_len_nan_row_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set selected sepal length to be Nan\n",
    "df.iloc[sepal_len_nan_row_idx.tolist(),0] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Nan\n",
    "# Question: Why this does not work? Have we not set the nan already\n",
    "df[df[\"sepal length (cm)\"] == np.nan] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us check the Nan in a different way\n",
    "df.iloc[sepal_len_nan_row_idx.tolist()] #Indeed they are set to nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct way of checking for nan\n",
    "\n",
    "df[df[\"sepal length (cm)\"].isnull()] #isnull and isna are equal. both work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set some sepal width also to be Nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sepal_wid_nan_row_idx = np.random.randint(low=0, high=df.shape[0], size=10)\n",
    "df.iloc[sepal_wid_nan_row_idx.tolist(),1] = np.nan\n",
    "df[df[\"sepal width (cm)\"].isnull()] #isnull and isna are equal. both work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic EDA\n",
    "\n",
    "### 1. Nan check in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional arguments do display all columns even when column count is large\n",
    "df.info(verbose=True, show_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().any(axis=0) # Columns that have ALL non null values show as false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns Series with True for rows that contain AT LEAST ONE na in columns\n",
    "df.isna().any(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum() # Check null count in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().any(axis=1).sum() #Total number of rows with Nan in any of four columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Duplicate check & immediate deletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a series with True False for every row\n",
    "dupsSeries = df.duplicated() \n",
    "print(f\"Number of duplicates = {dupsSeries.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)\n",
    "df.info() # check again to confirm duplicates gone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Delete features with zero or low variance\n",
    "\n",
    "A quasi-constant feature, using a threshold of 0.1 means 90% of the values are similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "var_thr = VarianceThreshold(threshold = 0.25) #Removing both constant and quasi-constant\n",
    "var_thr.fit(df)\n",
    "\n",
    "var_thr.get_support() # we are not deleting here. we just want to get an idea\n",
    "\n",
    "#also the threshold is generally less than 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Option 1 - Null value deletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the iris dataframe and set null values as before\n",
    "df, y = get_iris_df()\n",
    "df.iloc[sepal_len_nan_row_idx.tolist(),0] = np.nan\n",
    "df.iloc[sepal_wid_nan_row_idx.tolist(),1] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\n",
    "    df[\"sepal length (cm)\"].isnull() | \n",
    "    df[\"sepal width (cm)\"].isnull()\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop rows containing null value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conditionally dropping records\n",
    "df = df.drop(\n",
    "        df[\n",
    "            df[\"sepal length (cm)\"].isnull() | \n",
    "            df[\"sepal width (cm)\"].isnull()\n",
    "        ].index)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following is equivalent to drop all na rows in place\n",
    "#df.dropna(how = 'any', inplace = True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Option 2 - Mean/Median/Mode imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the iris dataframe and set null values as before\n",
    "df, y = get_iris_df()\n",
    "df.iloc[sepal_len_nan_row_idx.tolist(),0] = np.nan\n",
    "df.iloc[sepal_wid_nan_row_idx.tolist(),1] = np.nan\n",
    "df[\n",
    "    df[\"sepal length (cm)\"].isnull() | \n",
    "    df[\"sepal width (cm)\"].isnull()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean imputation directly from Pandas should also work\n",
    "# df['sepal length (cm)'] = df['sepal length (cm)'].fillna(df['sepal length (cm)'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean imputation for numeric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.mean() #Find mean of each column in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# we can use mean median or mode for imputation\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy=\"mean\") \n",
    "imputer.fit_transform(df) #did you notice the output is no longer dataframe? It is a numpy array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the previous cell imputed data, why are there still Nans in dataframe?\n",
    "Ans: Because imputation transforms are not in-place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\n",
    "    df[\"sepal length (cm)\"].isnull() | \n",
    "    df[\"sepal width (cm)\"].isnull()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(missing_values=np.nan, strategy=\"mean\") \n",
    "X_imputed = imputer.fit_transform(df)\n",
    "X_imputed[0:5,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. How to know which row/col is imputed?\n",
    "2. Why do we want to know which row/col is imputed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(missing_values=np.nan, strategy=\"mean\", add_indicator=True) \n",
    "X_imputed = imputer.fit_transform(df)\n",
    "X_imputed[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_imputed[\n",
    "    X_imputed[:,4] == 1 or X_imputed[:,5] == 1\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/10062954/valueerror-the-truth-value-of-an-array-with-more-than-one-element-is-ambiguous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a True false 1D array\n",
    "X_imputed[:,4] == 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does it make sense now why numpy wants clarity on what you plan to perform with OR between two True/False arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.logical_or(X_imputed[:,4] == 1, X_imputed[:,5] == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.any([X_imputed[:,4] == 1, X_imputed[:,5] == 1], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use logical or\n",
    "X_imputed[\n",
    "    np.logical_or(X_imputed[:,4] == 1, X_imputed[:,5] == 1)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Option 3 - Grouped Mean imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the iris dataframe and set null values as before\n",
    "df = get_iris_df(concatenate_Xy=True) #Note that now, we are including both X and y in df\n",
    "df.iloc[sepal_len_nan_row_idx.tolist(),0] = np.nan\n",
    "df.iloc[sepal_wid_nan_row_idx.tolist(),1] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp = df.groupby(\"target\")\n",
    "grp.get_group(0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp.get_group(1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp.get_group(2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"sepal length (cm)\"].isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"sepal_length_nan_indicator\"] = df[\"sepal length (cm)\"].isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"sepal_length_nan_indicator\"] = df[\"sepal length (cm)\"].isna().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"sepal_width_nan_indicator\"] = df[\"sepal width (cm)\"].isna().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sepal length (cm)'] = \\\n",
    "    df.groupby(['target'], sort=False)['sepal length (cm)'].apply(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "df['sepal width (cm)'] = \\\n",
    "    df.groupby(['target'], sort=False)['sepal width (cm)'].apply(lambda x: x.fillna(x.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Knn Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the iris dataframe and set null values as before\n",
    "df = get_iris_df(concatenate_Xy=True) #Note that now, we are NOT including y because KnnImputer will use y in distance calc\n",
    "df.iloc[sepal_len_nan_row_idx.tolist(),0] = np.nan\n",
    "df.iloc[sepal_wid_nan_row_idx.tolist(),1] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=2, weights='uniform', metric='nan_euclidean')\n",
    "X_imputed = imputer.fit_transform(df)\n",
    "X_imputed[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Multivariate imputations are better - seeing in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dummy = pd.DataFrame()\n",
    "df_dummy[\"Age\"] = [np.nan, 35, 45, 25, 55, 40, 30]\n",
    "df_dummy[\"MonthlySal_InLakhs\"] = [1, 3, 5, 1.2, 6.5, 4, 2.7]\n",
    "df_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "mean_imputed = imputer.fit_transform(df_dummy)\n",
    "df_mean_imputed = pd.DataFrame(mean_imputed, columns=[\"Age\", \"MonthlySal_InLakhs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "imputer = IterativeImputer()\n",
    "iter_imputed = imputer.fit_transform(df_dummy)\n",
    "df_iter_imputed = pd.DataFrame(iter_imputed, columns=[\"Age\", \"MonthlySal_InLakhs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, sharex=True, figsize=(12, 6))\n",
    "axes = np.reshape(axes, -1)\n",
    "\n",
    "dfs = [df_mean_imputed, df_iter_imputed]\n",
    "titles = ['Mean Imputation', 'Iterative Imputation']\n",
    "\n",
    "for i, df in enumerate(dfs):\n",
    "    # Plotting the data\n",
    "    x = df.Age\n",
    "    y = df.MonthlySal_InLakhs\n",
    "\n",
    "    sns.scatterplot(df, x=\"Age\", y=\"MonthlySal_InLakhs\", ax=axes[i], color='green')\n",
    "    \n",
    "    # Fitting and plotting a linear regression line\n",
    "    m, b = np.polyfit(x, y, 1)\n",
    "    linreg = m*x + b\n",
    "    axes[i].plot(x, linreg, color='black')\n",
    "    \n",
    "    # # Setting the titles and including the RMSE values\n",
    "    axes[i].set_title(titles[i], fontsize=16, fontweight='bold')\n",
    "    rmse = round(mean_squared_error(y, linreg, squared=False), 3)\n",
    "    print((rmse))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Multiple rounds of imputation\n",
    "\n",
    "This refers to the chicken and egg problem we talked in the class\n",
    "\n",
    "1. First we could do a mean imputation (or grouped mean imputation) by also using missing_indicator= true\n",
    "2. This flag ensures that a new feature is added that holds 0/1 values. 1 for rows whose Nan was imputed by mean. 0 otherwise\n",
    "3. With the nasty nans out of the way, apply scaling\n",
    "4. Now we could go back and run a pandas update (apply or other means) to replace all those rows with Nan again for which missing indicator feature is 1.\n",
    "5. Then throw away the missing indicator feature\n",
    "6. Apply a multivariate imputation like knn imputation for the Nans themselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing income dataset\n",
    "\n",
    "### 1. Deduplication & Nan Imputation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/income.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there no Nans?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: What will you check in csv for Nan? \n",
    "df.isin([?]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fix the code below\n",
    "df.workclass = df['workclass'].replace(?, np.nan)\n",
    "df.occupation = df.occupation.replace(?, np.nan)\n",
    "df['native-country'] = df[?].replace(?, ?) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual pleasure for your eyes only \n",
    "sns.heatmap(df.isnull());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_percent  = (df.isna().sum() * 100 / len(df)).round(2)\n",
    "nan_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There are {df.duplicated().sum()} duplicated values present.\")\n",
    "df[df.duplicated()].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Checking for numerical columns - Imputation & Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select numeric columns\n",
    "\n",
    "numeric_cols = df.select_dtypes(include=['int', 'float']).columns.tolist()\n",
    "print(numeric_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe stats can run only on numerical column. meaningless for others\n",
    "df[numeric_cols].describe() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"educational-num\"].unique() #this is really a numerical looking ordinal field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[numeric_cols].loc[:, df[numeric_cols].columns != \"educational-num\"]\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply Standard Scaler on numerical columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_numeric_scaled = sc.fit_transform(df2)\n",
    "X_numeric_scaled[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise\n",
    "\n",
    "1. Add code for imputing numerical columns if any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What will you pass as input? df2 or X_numeric_scaled?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Dealing with categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.nunique() # if the column listed takes on a fixed set of values then it is encoding candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.workclass.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"workclass\"].value_counts() # do for every candidate column & decide label, ordinal or one hot encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.education.unique()\n",
    "#df['marital-status'].value_counts()\n",
    "#df.occupation.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1 Impute Categorical column with most frequent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question: Why does this fail?\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "workclass_imputed = imputer.fit_transform(df[\"workclass\"])\n",
    "workclass_imputed[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fix the code below for the error you see \n",
    "imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "workclass_imputed = imputer.fit_transform(df[\"workclass\"]) #Fix this line of code \n",
    "workclass_imputed[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "df_categorical = df[categorical_columns]\n",
    "df_categorical.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_imputed = imputer.fit_transform(df_categorical)\n",
    "categorical_imputed[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we forgot to add educational-num to categorical dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "edu_num_imputed = imputer.fit_transform(df[\"educational-num\"].to_numpy().reshape(-1,1))\n",
    "edu_num_imputed[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2 Combine the categorical columns together\n",
    "\n",
    "before we stack all together, we should separate the income field in categorical. It is the target variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_categorical_features = categorical_imputed[:,:-1]\n",
    "\n",
    "target_var = categorical_imputed[:,-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_categorical = np.hstack((edu_num_imputed, imputed_categorical_features))\n",
    "X_categorical[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.3 Apply One Hot encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_onehot = np.hstack( (X_categorical[:,:6], X_categorical[:,-1:]) )\n",
    "X_onehot[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.4 Apply Label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gender and race columns are to be label encoded\n",
    "X_label = X_categorical[:,-3:-1]\n",
    "X_label[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "..\n",
    "..\n",
    "??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.5 Apply Ordinal Encoder to institutional-num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "..\n",
    "..\n",
    "??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.6 What about the target income? Is it Label or ordinal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Combine all features back together - Categorical and numerical\n",
    "\n",
    "Rememeber the order "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Apply KNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Do train and test split and do fit and predict  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What about train test split? What about fit_transform() and transform()? ... Oops**\n",
    "\n",
    "BIGGEST MISTAKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quickstart",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
